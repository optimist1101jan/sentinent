================================================================================
PERSISTENT CHARACTER AGENT
================================================================================

A persistent conversational AI with memory, identity, and time awareness.
Powered by Google Gemini API (gemma-3-12b-it) with real-time streaming responses.

--------------------------------------------------------------------------------
FILE STRUCTURE
--------------------------------------------------------------------------------

project/
├── main.py                 # Entry point (CLI loop, 5-turn cycles, streaming)
├── setup.py                # Initialize dirs, lore files, load episodes
├── manage_memory.py        # Memory pruning utility
├── model_config.py         # LLM configuration (Gemini API settings)
├── requirements.txt        # Python dependencies
│
├── agent/                  # State + Retrieval
│   ├── temporal.py         # Time delta calculation
│   ├── memory.py           # SQLite FTS5 storage
│   ├── semantic_search.py  # FAISS vector search (nomic-embed-text GGUF)
│   ├── conversation.py     # Session-based logging + buffer management
│   ├── dynamic_lore.py     # Dynamic lore retrieval using semantic search
│   ├── brain.db            # SQLite database
│   ├── semantic.index      # FAISS index
│   ├── semantic_chunks.json # Index mapping
│   ├── timestamps.json     # Last interaction timestamp
│   ├── lore/               # Static personality
│   │   ├── self.md
│   │   ├── user.md
│   │   └── relationship.md
│   ├── lore_index/         # Lore chunk index
│   │   └── chunks.json
│   └── episodes/           # Raw memory source
│       ├── ep_001.txt
│       └── ingest.py
│
├── pipeline/               # Runtime processing
│   ├── packet_builder.py   # Assemble XML-tagged prompt
│   ├── renderer_base.py    # Shared API utilities (SOLID)
│   ├── renderer.py         # Google Gemini API integration (non-streaming)
│   ├── summarizer_builder.py   # 5-turn summarization + indexing
│   ├── packet.md           # Current prompt
│   └── summarizer.md       # Current summarizer prompt
│
├── streaming/              # Streaming response handling
│   ├── __init__.py         # Module exports
│   └── renderer_streaming.py   # Real-time streaming with typewriter effect
│
├── proximity/              # Physical/emotional proximity detection
│   ├── __init__.py         # Module exports
│   └── proximity_manager.py    # Nomic-based proximity state detection
│
├── memory/                 # Long-term memory storage
│   ├── memory_loader.py    # Memory intent detection + fetching
│   ├── episodic/           # Episode summaries
│   └── semantic/           # Semantic memory files
│       └── memory.md
│
├── tools/                  # Utility scripts
│   └── index_lore.py       # Rebuild lore index
│
└── data/
    ├── nomic-embed-text-v1.5.Q8_0.gguf   # Embedding model (GGUF)
    ├── logs_raw/           # Conversation history - session-based files
    └── buffer/             # Session buffer backups

--------------------------------------------------------------------------------
DATA FLOW
--------------------------------------------------------------------------------

User Input → HOLD (temp RAM, not logged)
    ↓
Build Packet (pipeline/packet_builder.py) - XML format
    - Proximity state changed? → Inject from proximity/proximity_manager.py
    - Memory intent detected? → Fetch from memory/memory_loader.py
    ↓
Send to Google Gemini API (gemma-3-12b-it)
    ↓
Stream Response (streaming/renderer_streaming.py)
    - Typewriter effect: 15ms per character
    - Prefix "[AI]:" stripped during streaming
    - Real-time display as chunks arrive
    ↓
Validate → Check response validity
    ↓
Valid? → COMMIT (log both messages, increment counter)
Invalid? → DISCARD (nothing saved, retry)
    ↓
Turn == 5? → Summarize → Index to brain.db + FAISS → Reset

--------------------------------------------------------------------------------
COMPONENT VERIFICATION
--------------------------------------------------------------------------------

AGENT MODULE (agent/)
  temporal.py         [OK] TimeManager - time delta calculation
  memory.py           [OK] MemoryStore - SQLite FTS5 storage
  semantic_search.py  [OK] FAISS vector search (nomic-embed-text-v1.5, 768-dim)
  conversation.py     [OK] Session-based logging (convo_YYYY-MM-DD_HH-MM-SS.txt)

PIPELINE MODULE (pipeline/)
  packet_builder.py   [OK] XML-tagged packet generation
  renderer_base.py    [OK] Shared API utilities (clean, validate, payload)
  renderer.py         [OK] Google Gemini API integration with caching
  summarizer_builder.py [OK] 5-turn cycle summarization via Gemini API

MEMORY MODULE (memory/)
  memory_loader.py    [OK] Intent detection + fetching from episodic/semantic

STREAMING MODULE (streaming/)
  renderer_streaming.py [OK] Real-time streaming with typewriter effect

PROXIMITY MODULE (proximity/)
  proximity_manager.py  [OK] Nomic-based state detection (PHYSICAL/REMOTE/TRANSITION)

MAIN MODULE (main.py)
  Traffic Control     [OK] Hold-Wait-Commit pattern
  Session Init        [OK] New session file per run
  5-Turn Cycle        [OK] Auto-summarize after 5 turns
  Streaming Display   [OK] Character-by-character typewriter effect

--------------------------------------------------------------------------------
PACKET FORMAT (XML)
--------------------------------------------------------------------------------

<system_directive>
  Roleplay as AI...
  <persona>Name, relationship, identity, traits</persona>
  <lore>Static personality from lore files</lore>
</system_directive>

<context>
  <temporal_data>Current date, time since last chat</temporal_data>
  <memory_bank>Retrieved semantic + episodic memories (conditional)</memory_bank>
  <chat_history>Last 6 turns of conversation</chat_history>
  <distance_context>Physical/emotional proximity</distance_context>
</context>

<user_input>Current user message</user_input>

<mood>Current emotional state</mood>

<trigger>Execution command for response generation</trigger>

--------------------------------------------------------------------------------
RENDERER (pipeline/renderer.py)
--------------------------------------------------------------------------------

Google Gemini API implementation for Gemma models (non-streaming fallback).

Process:
  1. Parse XML sections → Build Gemini API payload
  2. Check cache first (deduplication)
  3. Send with retry logic (max 3 attempts)
  4. Clean: Strip [AI] prefixes, punctuation artifacts
  5. Validate: Check for impersonation, empty responses
  6. Cache successful responses
  7. Return cleaned text or fallback

--------------------------------------------------------------------------------
STREAMING RENDERER (streaming/renderer_streaming.py)
--------------------------------------------------------------------------------

Real-time streaming implementation with typewriter effect.

Process:
  1. Parse XML sections → Build Gemini API payload
  2. Send streaming request (streamGenerateContent endpoint)
  3. Collect all chunks → Detect prefix to strip
  4. Print with typewriter delay (15ms per char)
  5. Return cleaned full response

API Config:
  Model:           gemma-3-12b-it
  API:             Google Gemini API (v1beta)
  Endpoint:        streamGenerateContent
  Temperature:     0.7
  Max Tokens:      1000
  Timeout:         60s
  Char Delay:      0.015s (15ms) - typewriter effect

Fallback: "*AI looks at you, seemingly lost in thought, and doesn't respond.*"

--------------------------------------------------------------------------------
MEMORY SYSTEM
--------------------------------------------------------------------------------

The agent uses a **3-stage memory pipeline**:

### Stage 1 — Session Buffer
Raw conversation turns held in-memory for the current 5-turn cycle.

### Stage 2 — Summarization
After 5 turns, the buffer is sent to Gemini for compression into a single factual sentence.

### Stage 3 — Long-Term Indexing
The compressed memory is simultaneously indexed into:
- **Episodic Store** (SQLite FTS5) — keyword searchable
- **Semantic Index** (FAISS) — embedding-based similarity search

### Memory Retrieval
When the user asks a memory-related question (e.g., *"do you remember..."*), the system:
1. Detects memory intent via keyword patterns
2. Searches both episodic and semantic stores
3. Injects relevant memories into the prompt

--------------------------------------------------------------------------------
PROXIMITY SYSTEM
--------------------------------------------------------------------------------

Manager:             proximity/proximity_manager.py - State detection + injection
Embedding Model:     nomic-embed-text-v1.5.Q8_0.gguf (shared with semantic search)

States:
  PHYSICAL:          User is physically present, sitting together
  REMOTE:            User is chatting remotely (text, phone, discord)
  TRANSITION_AWAY:   User is leaving, saying goodbye
  TRANSITION_TOWARD: User is arriving, coming closer

Injection Logic:
  - First turn:      ALWAYS inject proximity context
  - State changed:   Inject new context
  - No change:       Block disappears from packet (saves tokens)

Detection:
  - Cosine similarity between user input and anchor embeddings
  - Confidence threshold: 0.45
  - Transitions map to final states (AWAY→REMOTE, TOWARD→PHYSICAL)

--------------------------------------------------------------------------------
TRAFFIC CONTROL
--------------------------------------------------------------------------------

Hold-Wait-Commit:
  HOLD:   Don't log user input immediately
  WAIT:   Get AI response → Clean → Validate
  COMMIT: Only save if valid (both messages together)
  DISCARD: If invalid, nothing saved (clean retry)

--------------------------------------------------------------------------------
LOGGING FORMAT
--------------------------------------------------------------------------------

Session File: convo_YYYY-MM-DD_HH-MM-SS.txt

# Conversation Session Started: 2026-02-06T11:30:45
#======================================================================

[11:30:52] user: Hello
[11:30:58] assistant: Hello! How can I help you today?

New file created for each conversation session.
Buffer Directory: data/buffer/ (session backup)

--------------------------------------------------------------------------------
USAGE
--------------------------------------------------------------------------------

Setup:
  python setup.py                       # Initialize project

Chat:
  python main.py                        # Run chat loop (streaming enabled)

Memory Management:
  python manage_memory.py list          # View all memories
  python manage_memory.py delete <id>   # Delete memory
  python manage_memory.py stats         # Show statistics
  python manage_memory.py rebuild       # Rebuild FAISS index
  python manage_memory.py clear         # Clear all memories

--------------------------------------------------------------------------------
SPECS
--------------------------------------------------------------------------------

Model:              gemma-3-12b-it
API:                Google Gemini API (v1beta)
Streaming:          Enabled (streaming/renderer_streaming.py)
Typewriter Effect:  15ms per character
Temperature:        0.7
Max Tokens:         1000
Timeout:            60s
Max Retries:        3
Embedding:          nomic-embed-text-v1.5.Q8_0.gguf (768-dim)
Embedding Library:  llama-cpp-python
Vector Index:       FAISS IndexFlatIP
Turn Cycle:         5 turns → summarize → index
Output Format:      XML-tagged packet
Log Format:         Session-based (convo_YYYY-MM-DD_HH-MM-SS.txt)
Cache Location:     ~/.cache/ai/responses/

